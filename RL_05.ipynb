{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RL Course by David Silver\n",
    "\n",
    "## Lecture 5 Model-Free Control\n",
    "\n",
    "- On-Policy Monte-Carlo Control\n",
    "- On-Policy Temporal-Difference Learing\n",
    "- Off-Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Monte-Carlo Control\n",
    "\n",
    "<img src=\"img/04_MC control.png\" width=60%>\n",
    "\n",
    "采用$\\epsilon-greedy$方式学习的最终策略, 依然是一个$epsilon-greedy$的策略. 如何保证不会丢到一个更好的策略,同时当我们得到一个当前的最好策略不想继续探索的时候, 得到的这个策略是一个确定性策略,而不包含随机行为? 由此引出了GLIE的概念.\n",
    "\n",
    "GLIE(greedy in limit with Infinite Exploration)要求学习方法有两个条件\n",
    "- 充分探索: 当迭代次数趋向无穷时, 所有状态-动作对都被探索无数次\n",
    "- 最优收敛: 策略收敛在一个贪婪策略,不再具有随机性\n",
    "\n",
    "满足GLIE的方法, 就能够达到我们上述的学习目的. \n",
    "对于$\\epsilon-greedy$方法而言, 当$\\epsilon = \\frac{1}{k}$时, 就满足GLIE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_greedy_policy(Q, epsilon, nA):\n",
    "    def policy(state):\n",
    "        prob = np.random.uniform()\n",
    "        if prob < 1 - epsilon:\n",
    "            return np.argmax(Q[state])\n",
    "        else:\n",
    "            return np.random.randint(nA)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLIE Monte-Carlo Control\n",
    "from collections import deque, Counter, defaultdict\n",
    "\n",
    "\n",
    "def GLIE_MC(MDP, episods, gamma = 1.):\n",
    "    '''GLIE Monte-Carlo control method. Use epsilon-greedy for improvement with a decent epsilon.\n",
    "    Params:\n",
    "        MDP: an instance of MDP\n",
    "        episods: number of episod\n",
    "        gamma: decay of reward\n",
    "    \n",
    "    Return:\n",
    "        Q, A defaultdict(np.ndarray)\n",
    "        Q[state][value]: the estimate expect of state-action pair\n",
    "    '''\n",
    "    N = defaultdict(lambda: np.zeros(len(MDP.action_space)))\n",
    "    Q = defaultdict(lambda: np.zeros(len(MDP.action_space)))\n",
    "    \n",
    "    for i_episod in range(1, episods+1):\n",
    "        if i_episod % 100000 == 0:\n",
    "            print(\"i_episod:%d/%d\"%(i_episod, episods))\n",
    "        episod = deque()\n",
    "        G = 0\n",
    "        s = MDP.reset()\n",
    "        policy = improve_greedy_policy(Q, 1./i_episod, len(MDP.action_space))\n",
    "        done = False\n",
    "        while not done: # run the simulation\n",
    "            a = policy(s)\n",
    "            s_n, r, done, _ = MDP.step(a)\n",
    "            episod.append((s, a, r))\n",
    "            s = s_n\n",
    "        while len(episod) != 0: # update value evaluation of every-visit\n",
    "            s, a, r = episod.pop()\n",
    "            G = r + gamma * G\n",
    "            N[s][a] += 1\n",
    "            Q[s][a] += (G - Q[s][a])/N[s][a]\n",
    "        policy = improve_greedy_policy(Q, 1./i_episod, len(MDP.action_space)) # update epsilon for descent.\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarsa Algorithm\n",
    "from collections import defaultdict\n",
    "\n",
    "def Sarsa(MDP, episods, gamma = 1, alpha = 0.1, epsilon = 0.3):\n",
    "    '''Sarsa Algorithm for On-policy Control\n",
    "    \n",
    "    Params:\n",
    "        MDP: an instance of MDP\n",
    "        episods: number of episod\n",
    "        gamma: decay of reward\n",
    "        alpha: parameter of TD-learning, learning rate\n",
    "        epsilon: parameter of epsilon-greedy\n",
    "        \n",
    "    Return:\n",
    "        Q, A defaultdict(np.ndarray)\n",
    "        Q[state][value]: the estimate expect of state-action pair\n",
    "    '''\n",
    "    Q = defaultdict(lambda: np.zeros(len(MDP.action_space)))\n",
    "    policy = improve_greedy_policy(Q, epsilon, len(MDP.action_space))\n",
    "    \n",
    "    for i_episod in range(1, episods+1):\n",
    "        if i_episod % 100000 == 0:\n",
    "            print(\"\\ri_episod:%d/%d\"%(i_episod, episods), end=\"\")\n",
    "        s = MDP.reset()\n",
    "        done = False\n",
    "        a = policy(s)\n",
    "        while not done:\n",
    "            s_next, r, done, _ = MDP.step(a)\n",
    "            if not done:\n",
    "                a_next = policy(s_next)\n",
    "                Q[s][a] += alpha * (r + Q[s_next][a_next] - Q[s][a])\n",
    "                s = s_next\n",
    "                a = a_next\n",
    "            else:\n",
    "                Q[s][a] += alpha * (r + 0 - Q[s][a]) # Q(terminate-state,.) = 0\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
